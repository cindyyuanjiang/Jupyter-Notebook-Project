{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import sklearn.naive_bayes as nb\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.layers import LSTM,Conv1D,MaxPooling1D\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense \n",
    "import tensorflow.keras.backend as K \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout,Input,Dense,Activation,Flatten,SeparableConv2D,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from sklearn.utils import resample\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "config = tf.ConfigProto() \n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9 \n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class load:\n",
    "    def __init__(self):\n",
    "        self.file_name_dir = []\n",
    "        self.total_data = []\n",
    "        self.total_label = []\n",
    "    \n",
    "    def load_file(self,dir_location):\n",
    "        print('now loading_file (location : ' + dir_location + ') ... \\n')\n",
    "        \n",
    "        for root,dirs,files in os.walk(dir_location):\n",
    "            for fname in files:\n",
    "                full_fname = os.path.join(root,fname)\n",
    "                self.file_name_dir.append(full_fname)\n",
    "        \n",
    "        print('make file list complete')\n",
    "    \n",
    "    def make_DataFrame(self,tar_li,p_n):\n",
    "        for file_name in tqdm(self.file_name_dir):\n",
    "            sp = file_name.split('/')\n",
    "            tmp_label = sp[1]\n",
    "            d = open(file_name,'r',encoding='UTF8').read()\n",
    "            data = d.split('\\n')\n",
    "            data.pop(0) # remove trash data header\n",
    "            index = data.pop(0)\n",
    "            tmp_real_data = []\n",
    "            for dat_num in range(len(data)):\n",
    "                if data[dat_num] == '':\n",
    "                    continue\n",
    "                tmp_real_data.append(data[dat_num].split(','))\n",
    "            \n",
    "            df = pd.DataFrame(tmp_real_data)\n",
    "            index_li = index.split(',')\n",
    "            df.columns = index_li\n",
    "            \n",
    "            #now change str to float\n",
    "            \n",
    "            for y in index_li:\n",
    "                df[y] = pd.to_numeric(df[y],downcast='float')\n",
    "            \n",
    "            tmp_li = []\n",
    "            for i in range(len(df)):\n",
    "                tmp = []\n",
    "                for j in tar_li:\n",
    "                    tmp.append((df[j][i]/1000)**p_n)\n",
    "                tmp_li.append(tmp)\n",
    "            \n",
    "            self.total_data.append(tmp_li)\n",
    "            self.total_label.append(tmp_label)\n",
    "        print('make total_data finish.....')\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    def return_data(self):\n",
    "        return self.total_data , self.total_label\n",
    "\n",
    "            \n",
    "\n",
    "         \n",
    "class Train_model:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.encoder = LabelEncoder()\n",
    "        self.enc_label = 0\n",
    "        \n",
    "        self.total_data = 0\n",
    "        self.total_label = 0\n",
    "        \n",
    "        self.x_train = 0\n",
    "        self.y_train = 0\n",
    "        self.x_test = 0\n",
    "        self.y_test = 0\n",
    "        \n",
    "        self.earlystopping = EarlyStopping(monitor='val_loss',patience=10)\n",
    "        \n",
    "        #model list\n",
    "        self.lstm = 0\n",
    "        self.svm = 0\n",
    "        self.xgboost = 0\n",
    "        self.nb = 0\n",
    "        self.rf =0\n",
    "        self.knn = 0\n",
    "        \n",
    "        #sample prediction\n",
    "        self.sample_data = 0\n",
    "        self.sample_label = 0\n",
    "        \n",
    "    def get_enc(self):\n",
    "        self.enc_label = self.encoder.fit_transform(self.total_label)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def make_arr(self):\n",
    "        self.total_data = np.array(self.total_data)\n",
    "        self.enc_label =np.array(self.enc_label)\n",
    "        \n",
    "    def Data_Augmentation(self,nu):\n",
    "        \n",
    "        #Data Augmentation is very optional Function\n",
    "        \n",
    "        tmp_li = [0]\n",
    "        for x in range(len(self.enc_label)-1):\n",
    "            if self.enc_label[x] != self.enc_label[x+1]:\n",
    "                tmp_li.append(x)\n",
    "        tmp_li.append(len(self.enc_label)-1)\n",
    "        print('Augmentation Data index is : ',tmp_li)\n",
    "        \n",
    "        div_data = []\n",
    "        div_label = []\n",
    "        for x in range(len(tmp_li)-1):\n",
    "            div_tmp =[]\n",
    "            div_la = []\n",
    "            for y in range(tmp_li[x]+1,tmp_li[x+1]+1):\n",
    "                div_tmp.append(self.total_data[y])\n",
    "                div_la.append(self.enc_label[y])\n",
    "            div_data.append(div_tmp)\n",
    "            div_label.append(div_la)\n",
    "        \n",
    "        \n",
    "        boot = []\n",
    "        label = []\n",
    "        t_li = [5,1,7,6,0,4,2,3]\n",
    "        for x in range(len(div_data)):\n",
    "            tmp = resample(div_data[x],replace=True,n_samples = nu,random_state=1)\n",
    "            boot+=tmp\n",
    "            label += list(t_li[x] for i in range(nu))\n",
    "        \n",
    "        boot = np.array(boot)\n",
    "        label = np.array(label)\n",
    "        \n",
    "        self.total_data = np.append(self.total_data,boot,axis=0)\n",
    "        self.enc_label = np.append(self.enc_label,label,axis=0)\n",
    "        \n",
    "    def divide_dataset(self,mode):\n",
    "        \n",
    "        if mode == 'lstm':\n",
    "            self.x_train,self.x_test,self.y_train,self.y_test = train_test_split(self.total_data,self.enc_label,test_size=0.2,random_state=0)\n",
    "\n",
    "        else:\n",
    "            self.x_train,self.x_test,self.y_train,self.y_test = train_test_split(self.total_data,self.enc_label,test_size=0.2,random_state=0)\n",
    "            nsamples,nx,ny = self.x_train.shape\n",
    "            self.x_train = self.x_train.reshape((nsamples,nx*ny))\n",
    "            nsamples,nx,ny = self.x_test.shape\n",
    "            self.x_test = self.x_test.reshape((nsamples,nx*ny))\n",
    "        \n",
    "    def model_create_train(self,mode):\n",
    "        if mode == 'lstm':\n",
    "            with tf.device('/GPU:0'):\n",
    "                model = Sequential() # Sequeatial Model \n",
    "                model.add(LSTM(180, input_shape=(60,3),return_sequences = True)) # (timestep, feature) \n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(Conv1D(128,\n",
    "                                 2,\n",
    "                                 padding='valid',\n",
    "                                 activation='relu',\n",
    "                                 strides=1))\n",
    "                model.add(MaxPooling1D(pool_size=4))\n",
    "                model.add(LSTM(128))\n",
    "                model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "                # 3. 모델 학습과정 설정하기\n",
    "                model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "                hist = model.fit(self.x_train, self.y_train, epochs=100, batch_size=256,callbacks=[self.earlystopping] ,validation_data=(self.x_test, self.y_test))\n",
    "                #model.save('model_x.h5')\n",
    "                #model.save_weights('model_x_weights.h5')\n",
    "            self.lstm = model\n",
    "            \n",
    "        elif mode=='svm':\n",
    "            #####here to change####\n",
    "            ######################################################################\n",
    "\n",
    "            best_score = 0\n",
    "\n",
    "            for gamma in [0.001,0.01,0.1,1,10]:\n",
    "                for C in [0.001,0.01,0.1,1,10]:\n",
    "                    for kernel in ['linear','rbf','poly']:\n",
    "                        tmp_model = svm.SVC(kernel=kernel,gamma=gamma,C=C)\n",
    "                        scores = cross_val_score(tmp_model,self.x_train,self.y_train,cv=10,n_jobs=-1)\n",
    "                        score = np.mean(scores)\n",
    "\n",
    "                        if score>best_score:\n",
    "\n",
    "                            best_score = score\n",
    "                            best_parameter = {'kernel':kernel,'gamma':gamma,'C':C}\n",
    "                            print('best_parameter is change : ',best_parameter)\n",
    "                        else:\n",
    "                            print('remain :',best_parameter)\n",
    "            mod = svm.SVC(**best_parameter)\n",
    "            \n",
    "\n",
    "            ######################################################################\n",
    "            \n",
    "            predict_model = mod.fit(self.x_train,self.y_train)\n",
    "            print('fitting ',mode,' is complete...')\n",
    "            print(mode,'score is :',predict_model.score(self.x_test,self.y_test))\n",
    "\n",
    "            prediction = predict_model.predict(self.x_test)\n",
    "            self.svm = mod\n",
    "        \n",
    "        elif mode=='xgboost':\n",
    "            #####here to change####\n",
    "            ######################################################################\n",
    "\n",
    "            space={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n",
    "                    'gamma': hp.uniform ('gamma', 1,9),\n",
    "                    'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n",
    "                    'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "                    'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
    "                    'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "                    'n_estimators': 180,\n",
    "                    'seed': 0\n",
    "                }\n",
    "\n",
    "            mod =xgb.XGBClassifier(\n",
    "                                n_estimators =space['n_estimators'], max_depth = space['max_depth'], gamma = space['gamma'],\n",
    "                                reg_alpha = space['reg_alpha'],min_child_weight=space['min_child_weight'],\n",
    "                                colsample_bytree=space['colsample_bytree'])\n",
    "\n",
    "            \n",
    "\n",
    "            ######################################################################\n",
    "            \n",
    "            predict_model = mod.fit(self.x_train,self.y_train)\n",
    "            print('fitting ',mode,' is complete...')\n",
    "            print(mode,'score is :',predict_model.score(self.x_test,self.y_test))\n",
    "\n",
    "            prediction = predict_model.predict(self.x_test)\n",
    "            self.xgboost = mod\n",
    "        \n",
    "        elif mode=='nb':\n",
    "            #####here to change####\n",
    "            ######################################################################\n",
    "\n",
    "            mod = GaussianNB()\n",
    "\n",
    "            ######################################################################\n",
    "            \n",
    "            predict_model = mod.fit(self.x_train,self.y_train)\n",
    "            print('fitting ',mode,' is complete...')\n",
    "            print(mode,'score is :',predict_model.score(self.x_test,self.y_test))\n",
    "\n",
    "            prediction = predict_model.predict(self.x_test)\n",
    "            self.nb = mod\n",
    "        \n",
    "        \n",
    "        elif mode=='rf':\n",
    "            #####here to change####\n",
    "            ######################################################################\n",
    "            \n",
    "            rfc=RandomForestClassifier(random_state=42)\n",
    "            param_grid = { \n",
    "                'n_estimators': [10,15,20,30,40,50,100],\n",
    "                'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                'max_depth' : [3,4,5,6,7,8,9],\n",
    "                'criterion' :['gini', 'entropy']\n",
    "            }\n",
    "\n",
    "            mod = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 10,n_jobs=-1)\n",
    "\n",
    "            \n",
    "            ######################################################################\n",
    "            \n",
    "            predict_model = mod.fit(self.x_train,self.y_train)\n",
    "            print('fitting ',mode,' is complete...')\n",
    "            print(mode,'score is :',predict_model.score(self.x_test,self.y_test))\n",
    "\n",
    "            prediction = predict_model.predict(self.x_test)\n",
    "            self.rf = mod\n",
    "        \n",
    "        elif mode=='knn':\n",
    "            #####here to change####\n",
    "            ######################################################################\n",
    "            \n",
    "            leaf_size = list(range(1,30))\n",
    "            n_neighbors = list(range(1,8))\n",
    "            p=[1,2]\n",
    "\n",
    "            hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "\n",
    "            knn = KNeighborsClassifier()\n",
    "\n",
    "            mod = GridSearchCV(knn, hyperparameters, cv=10, n_jobs=-1)\n",
    "\n",
    "            \n",
    "            ######################################################################\n",
    "            \n",
    "            predict_model = mod.fit(self.x_train,self.y_train)\n",
    "            print('fitting ',mode,' is complete...')\n",
    "            print(mode,'score is :',predict_model.score(self.x_test,self.y_test))\n",
    "\n",
    "            prediction = predict_model.predict(self.x_test)\n",
    "            self.knn = mod\n",
    "        \n",
    "    \n",
    "    def prediction(self,input_axis,mode,p_n):\n",
    "        p = load()\n",
    "        p.load_file('test_data')\n",
    "        p.make_DataFrame(input_axis,p_n)\n",
    "        self.sample_data , self.sample_label = p.return_data()\n",
    "        \n",
    "        if mode == 'lstm':\n",
    "            self.sample_data = np.array(self.sample_data)\n",
    "            print('here is :',self.sample_data[0])\n",
    "            sample_pred = self.lstm.predict(self.sample_data)\n",
    "            sample_pred = np.argmax(sample_pred,axis=-1)\n",
    "            lab = self.encoder.inverse_transform(sample_pred)\n",
    "            \n",
    "            hit = 0\n",
    "            miss = 0\n",
    "            answer=[]\n",
    "            print('testing new data result :\\n[answer]  -->  [predict err]')\n",
    "            for x in range(len(lab)):\n",
    "                if lab[x] == self.sample_label[x]:\n",
    "                    hit+=1\n",
    "                    answer.append(lab[x])\n",
    "                else:\n",
    "                    miss+=1\n",
    "                    print(self.sample_label[x],' --> ' ,lab[x],'        err_index number : ',x)\n",
    "\n",
    "\n",
    "            print('hit: ',hit,' miss : ',miss,'percent : ',(100*hit)/(hit+miss))\n",
    "        \n",
    "        else:\n",
    "            model_list = ['svm','knn','rf','nb','xgboost']\n",
    "            match_list = [self.svm , self.knn , self.rf , self.nb , self.xgboost]\n",
    "            \n",
    "            for x in range(len(model_list)):\n",
    "                if model_list[x] == mode:\n",
    "                    mod = match_list[x]\n",
    "                    print(mode + 'model match complete.....')\n",
    "                \n",
    "            self.sample_data = np.array(self.sample_data)\n",
    "            nsamples , nx , ny = self.sample_data.shape\n",
    "            sample = self.sample_data.reshape((nsamples,nx*ny))\n",
    "            print('here is :',sample[0])\n",
    "            sample_pred = mod.predict(sample)\n",
    "            lab = self.encoder.inverse_transform(sample_pred)\n",
    "            \n",
    "            hit = 0\n",
    "            miss = 0\n",
    "            answer=[]\n",
    "            print('testing new data result :\\n[answer]  -->  [predict err]')\n",
    "            for x in range(len(lab)):\n",
    "                if lab[x] == self.sample_label[x]:\n",
    "                    hit+=1\n",
    "                    answer.append(lab[x])\n",
    "                else:\n",
    "                    miss+=1\n",
    "                    print(self.sample_label[x],' --> ' ,lab[x],'        err_index number : ',x)\n",
    "\n",
    "\n",
    "            print('hit: ',hit,' miss : ',miss,'percent : ',(100*hit)/(hit+miss))\n",
    "            \n",
    "    \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IO(tar_dir,input_axis,p_n):\n",
    "    total_dat = []\n",
    "    total_lab = []\n",
    "    v = load()\n",
    "    v.load_file(tar_dir)\n",
    "    v.make_DataFrame(input_axis,p_n)\n",
    "    total_dat,total_lab = v.return_data()\n",
    "    return total_dat , total_lab\n",
    "\n",
    "def pipline(total_data,total_label,input_axis,mode,p_n,aug):\n",
    "    t = Train_model()\n",
    "    t.total_data = total_data\n",
    "    t.total_label = total_label\n",
    "    t.get_enc()\n",
    "    t.make_arr()\n",
    "    if aug != 0:\n",
    "        t.Data_Augmentation(5000)\n",
    "    t.divide_dataset(mode)\n",
    "    t.model_create_train(mode)\n",
    "    t.prediction(input_axis,mode,p_n)\n",
    "\n",
    "    \n",
    "def prac_machine(tar_dir,input_axis,mode_name,power,is_aug):\n",
    "    total_data = []\n",
    "    total_label = []\n",
    "    print('Dir : '+tar_dir+'\\nthis ML model name is '+mode_name+'\\npower : '+str(power),'\\n\\n\\n')\n",
    "    total_data,total_label = IO(tar_dir,input_axis,power)\n",
    "    pipline(total_data,total_label,input_axis,mode_name,power,is_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/12331 [00:00<01:12, 169.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir : swing\n",
      "this ML model name is lstm\n",
      "power : 4 \n",
      "\n",
      "\n",
      "\n",
      "now loading_file (location : swing) ... \n",
      "\n",
      "make file list complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12331/12331 [01:13<00:00, 168.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make total_data finish.....\n",
      "Augmentation Data index is :  [0, 1838, 3407, 5041, 6741, 8170, 9491, 10885, 12330]\n",
      "Train on 41864 samples, validate on 10467 samples\n",
      "Epoch 1/100\n",
      "41864/41864 [==============================] - 10s 245us/sample - loss: 0.4213 - acc: 0.8484 - val_loss: 0.1543 - val_acc: 0.9458\n",
      "Epoch 2/100\n",
      "41864/41864 [==============================] - 10s 228us/sample - loss: 0.1284 - acc: 0.9574 - val_loss: 0.0993 - val_acc: 0.9675\n",
      "Epoch 3/100\n",
      "41864/41864 [==============================] - 10s 227us/sample - loss: 0.0816 - acc: 0.9738 - val_loss: 0.0618 - val_acc: 0.9821\n",
      "Epoch 4/100\n",
      "41864/41864 [==============================] - 9s 226us/sample - loss: 0.0527 - acc: 0.9828 - val_loss: 0.0727 - val_acc: 0.9777\n",
      "Epoch 5/100\n",
      "41864/41864 [==============================] - 9s 226us/sample - loss: 0.0476 - acc: 0.9846 - val_loss: 0.0534 - val_acc: 0.9836\n",
      "Epoch 6/100\n",
      "41864/41864 [==============================] - 10s 228us/sample - loss: 0.0339 - acc: 0.9895 - val_loss: 0.0407 - val_acc: 0.9875\n",
      "Epoch 7/100\n",
      "41864/41864 [==============================] - 10s 227us/sample - loss: 0.0265 - acc: 0.9916 - val_loss: 0.0335 - val_acc: 0.9894\n",
      "Epoch 8/100\n",
      "41864/41864 [==============================] - 9s 227us/sample - loss: 0.0264 - acc: 0.9916 - val_loss: 0.0198 - val_acc: 0.9949\n",
      "Epoch 9/100\n",
      "41864/41864 [==============================] - 10s 227us/sample - loss: 0.0194 - acc: 0.9938 - val_loss: 0.0158 - val_acc: 0.9962\n",
      "Epoch 10/100\n",
      "41864/41864 [==============================] - 10s 228us/sample - loss: 0.0168 - acc: 0.9944 - val_loss: 0.0213 - val_acc: 0.9930\n",
      "Epoch 11/100\n",
      "41864/41864 [==============================] - 10s 228us/sample - loss: 0.0125 - acc: 0.9964 - val_loss: 0.0140 - val_acc: 0.9960\n",
      "Epoch 12/100\n",
      "41864/41864 [==============================] - 10s 228us/sample - loss: 0.0152 - acc: 0.9955 - val_loss: 0.0202 - val_acc: 0.9942\n",
      "Epoch 13/100\n",
      "41864/41864 [==============================] - 10s 227us/sample - loss: 0.0142 - acc: 0.9956 - val_loss: 0.0154 - val_acc: 0.9960\n",
      "Epoch 14/100\n",
      "41864/41864 [==============================] - 9s 227us/sample - loss: 0.0127 - acc: 0.9961 - val_loss: 0.0109 - val_acc: 0.9973\n",
      "Epoch 15/100\n",
      "41864/41864 [==============================] - 10s 228us/sample - loss: 0.0085 - acc: 0.9973 - val_loss: 0.0125 - val_acc: 0.9970\n",
      "Epoch 16/100\n",
      "41864/41864 [==============================] - 9s 227us/sample - loss: 0.0078 - acc: 0.9976 - val_loss: 0.0160 - val_acc: 0.9944\n",
      "Epoch 17/100\n",
      "41864/41864 [==============================] - 9s 227us/sample - loss: 0.0141 - acc: 0.9957 - val_loss: 0.0204 - val_acc: 0.9935\n",
      "Epoch 18/100\n",
      "41864/41864 [==============================] - 10s 227us/sample - loss: 0.0092 - acc: 0.9970 - val_loss: 0.0090 - val_acc: 0.9975\n",
      "Epoch 19/100\n",
      "41864/41864 [==============================] - 10s 227us/sample - loss: 0.0039 - acc: 0.9991 - val_loss: 0.0097 - val_acc: 0.9975\n",
      "Epoch 20/100\n",
      "41864/41864 [==============================] - 10s 228us/sample - loss: 0.0083 - acc: 0.9978 - val_loss: 0.0321 - val_acc: 0.9903\n",
      "Epoch 21/100\n",
      "41864/41864 [==============================] - 10s 227us/sample - loss: 0.0188 - acc: 0.9942 - val_loss: 0.0208 - val_acc: 0.9939\n",
      "Epoch 22/100\n",
      "41864/41864 [==============================] - 9s 226us/sample - loss: 0.0110 - acc: 0.9965 - val_loss: 0.0082 - val_acc: 0.9977\n",
      "Epoch 23/100\n",
      "41864/41864 [==============================] - 9s 226us/sample - loss: 0.0047 - acc: 0.9985 - val_loss: 0.0077 - val_acc: 0.9984\n",
      "Epoch 24/100\n",
      "41864/41864 [==============================] - 10s 227us/sample - loss: 0.0020 - acc: 0.9995 - val_loss: 0.0062 - val_acc: 0.9987\n",
      "Epoch 25/100\n",
      "41864/41864 [==============================] - 10s 228us/sample - loss: 6.4725e-04 - acc: 0.9999 - val_loss: 0.0057 - val_acc: 0.9990\n",
      "Epoch 26/100\n",
      "41864/41864 [==============================] - 9s 227us/sample - loss: 0.0010 - acc: 0.9997 - val_loss: 0.0105 - val_acc: 0.9969\n",
      "Epoch 27/100\n",
      "41864/41864 [==============================] - 10s 228us/sample - loss: 0.0221 - acc: 0.9931 - val_loss: 0.0143 - val_acc: 0.9964\n",
      "Epoch 28/100\n",
      "41864/41864 [==============================] - 10s 227us/sample - loss: 0.0041 - acc: 0.9989 - val_loss: 0.0122 - val_acc: 0.9971\n",
      "Epoch 29/100\n",
      "41864/41864 [==============================] - 9s 227us/sample - loss: 0.0049 - acc: 0.9987 - val_loss: 0.0144 - val_acc: 0.9963\n",
      "Epoch 30/100\n",
      "41864/41864 [==============================] - 10s 228us/sample - loss: 0.0149 - acc: 0.9954 - val_loss: 0.0123 - val_acc: 0.9968\n",
      "Epoch 31/100\n",
      "41864/41864 [==============================] - 10s 227us/sample - loss: 0.0084 - acc: 0.9977 - val_loss: 0.0130 - val_acc: 0.9965\n",
      "Epoch 32/100\n",
      "41864/41864 [==============================] - 10s 227us/sample - loss: 0.0039 - acc: 0.9989 - val_loss: 0.0069 - val_acc: 0.9982\n",
      "Epoch 33/100\n",
      "41864/41864 [==============================] - 10s 228us/sample - loss: 0.0040 - acc: 0.9988 - val_loss: 0.0091 - val_acc: 0.9981\n",
      "Epoch 34/100\n",
      "41864/41864 [==============================] - 9s 227us/sample - loss: 0.0042 - acc: 0.9988 - val_loss: 0.0066 - val_acc: 0.9982\n",
      "Epoch 35/100\n",
      "41864/41864 [==============================] - 10s 228us/sample - loss: 6.6251e-04 - acc: 0.9998 - val_loss: 0.0052 - val_acc: 0.9991\n",
      "Epoch 36/100\n",
      "41864/41864 [==============================] - 9s 227us/sample - loss: 1.5379e-04 - acc: 1.0000 - val_loss: 0.0049 - val_acc: 0.9990\n",
      "Epoch 37/100\n",
      "41864/41864 [==============================] - 9s 227us/sample - loss: 1.0052e-04 - acc: 1.0000 - val_loss: 0.0049 - val_acc: 0.9992\n",
      "Epoch 38/100\n",
      "41864/41864 [==============================] - 10s 227us/sample - loss: 7.3892e-05 - acc: 1.0000 - val_loss: 0.0049 - val_acc: 0.9992\n",
      "Epoch 39/100\n",
      "41864/41864 [==============================] - 10s 227us/sample - loss: 7.1330e-05 - acc: 1.0000 - val_loss: 0.0056 - val_acc: 0.9992\n",
      "Epoch 40/100\n",
      "41864/41864 [==============================] - 9s 226us/sample - loss: 5.4693e-05 - acc: 1.0000 - val_loss: 0.0052 - val_acc: 0.9992\n",
      "Epoch 41/100\n",
      "41864/41864 [==============================] - 9s 226us/sample - loss: 4.6212e-05 - acc: 1.0000 - val_loss: 0.0053 - val_acc: 0.9992\n",
      "Epoch 42/100\n",
      "41864/41864 [==============================] - 10s 227us/sample - loss: 4.0336e-05 - acc: 1.0000 - val_loss: 0.0052 - val_acc: 0.9992\n",
      "Epoch 43/100\n",
      "41864/41864 [==============================] - 9s 227us/sample - loss: 3.6442e-05 - acc: 1.0000 - val_loss: 0.0052 - val_acc: 0.9992\n",
      "Epoch 44/100\n",
      "41864/41864 [==============================] - 9s 227us/sample - loss: 3.2081e-05 - acc: 1.0000 - val_loss: 0.0053 - val_acc: 0.9992\n",
      "Epoch 45/100\n",
      "41864/41864 [==============================] - 10s 228us/sample - loss: 3.0194e-05 - acc: 1.0000 - val_loss: 0.0053 - val_acc: 0.9992\n",
      "Epoch 46/100\n",
      "41864/41864 [==============================] - 10s 228us/sample - loss: 2.5713e-05 - acc: 1.0000 - val_loss: 0.0054 - val_acc: 0.9992\n",
      "Epoch 47/100\n",
      "41864/41864 [==============================] - 10s 228us/sample - loss: 2.4267e-05 - acc: 1.0000 - val_loss: 0.0055 - val_acc: 0.9992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 17/87 [00:00<00:00, 167.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now loading_file (location : test_data) ... \n",
      "\n",
      "make file list complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:00<00:00, 170.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make total_data finish.....\n",
      "here is : [[4.17401244e-02 4.71199874e-03 3.91476714e-01]\n",
      " [4.21107337e-02 3.90625000e-03 4.32596914e-01]\n",
      " [2.38544936e-02 3.72209808e-03 5.39415334e-01]\n",
      " [1.91501315e-02 4.43076610e-03 6.41641051e-01]\n",
      " [1.67961600e-02 5.47363226e-03 6.05165750e-01]\n",
      " [1.99871734e-02 5.71914063e-03 8.00874647e-01]\n",
      " [2.17432719e-02 4.49986056e-03 1.06136355e+00]\n",
      " [2.48405969e-02 2.79841000e-03 9.37519682e-01]\n",
      " [1.58823006e-02 1.27598984e-03 8.81647760e-01]\n",
      " [5.71914063e-03 9.81506241e-04 1.12114426e+00]\n",
      " [2.17678234e-03 6.71898241e-04 1.34044527e+00]\n",
      " [3.66218626e-03 5.47981281e-04 1.41676886e+00]\n",
      " [4.43076610e-03 5.92240896e-04 1.24824533e+00]\n",
      " [2.51763098e-03 6.55360000e-04 1.44291988e+00]\n",
      " [1.66496642e-03 8.95745041e-04 1.50172525e+00]\n",
      " [2.01996314e-03 3.54453530e-03 1.15196430e+00]\n",
      " [1.56823920e-03 9.47585434e-03 9.56720691e-01]\n",
      " [3.54453530e-03 2.90937838e-02 8.88949151e-01]\n",
      " [1.04857600e-02 3.99236365e-02 6.97886477e-01]\n",
      " [1.46661788e-02 2.93765888e-02 5.72897610e-01]\n",
      " [3.14143721e-02 1.44590063e-03 6.79740887e-01]\n",
      " [4.55583411e-02 7.48052010e-05 1.58478893e+00]\n",
      " [1.06302734e-01 1.63617014e-01 2.54456414e+00]\n",
      " [7.25553483e-02 1.35043922e+00 1.22945740e+00]\n",
      " [1.58532182e-01 1.43112782e+01 2.31973236e-01]\n",
      " [1.43986856e-01 6.30614504e+01 5.29414857e-01]\n",
      " [2.17611987e-01 2.67186780e+02 1.78579390e+00]\n",
      " [3.96376643e+00 4.27088351e+02 1.32211505e-01]\n",
      " [9.06950714e+01 3.30574069e+02 1.12945882e-02]\n",
      " [1.27606868e+02 2.17432719e+02 1.14338110e-02]\n",
      " [3.59379830e+02 1.75359065e+02 3.72209808e-03]\n",
      " [9.11076029e+02 4.65471066e+01 3.61364892e-02]\n",
      " [1.18889688e+03 4.18201360e+01 1.55274029e-02]\n",
      " [1.42403958e+03 4.55571542e+01 1.92057804e-01]\n",
      " [1.18243111e+03 4.54170323e+01 8.01025846e-02]\n",
      " [3.71752372e+02 5.92141946e+01 4.70025421e-01]\n",
      " [1.96911505e+02 8.42889248e+01 6.38778184e-01]\n",
      " [1.26849222e+02 9.80814712e+01 8.21386940e-01]\n",
      " [9.59792496e+01 8.25226170e+01 1.56231001e+00]\n",
      " [5.64463861e+01 5.87884247e+01 1.82944290e+00]\n",
      " [2.00855363e+01 4.12312444e+01 1.82944290e+00]\n",
      " [9.50819426e+00 3.24102035e+01 1.81689102e+00]\n",
      " [7.42999109e+00 2.42017164e+01 1.39610530e+00]\n",
      " [9.50819426e+00 1.88259578e+01 2.03243800e+00]\n",
      " [7.77796321e+00 1.24389107e+01 1.81063936e+00]\n",
      " [4.45720344e+00 8.51042925e+00 7.00945701e-01]\n",
      " [2.86489815e+00 5.63910985e+00 1.41202341e-01]\n",
      " [1.84205970e+00 4.25223991e+00 2.22754737e-01]\n",
      " [1.89318061e+00 3.24347133e+00 1.72005950e-01]\n",
      " [2.98113375e+00 1.65952365e+00 1.08554435e-01]\n",
      " [2.90919416e+00 8.96295799e-01 5.00546654e-02]\n",
      " [2.52047376e+00 4.13711386e-01 1.47578906e-03]\n",
      " [1.31079601e+00 1.73076801e-01 2.14358881e-04]\n",
      " [1.17436451e+00 1.54550411e-01 3.42102016e-04]\n",
      " [1.13427612e+00 1.35754666e-01 1.04060401e-04]\n",
      " [1.11679242e+00 1.41202341e-01 6.56100000e-05]\n",
      " [9.88053892e-01 1.57529610e-01 5.72897610e-05]\n",
      " [8.14506250e-01 1.84062451e-01 8.49346560e-05]\n",
      " [6.79740887e-01 1.62590401e-01 3.89500810e-05]\n",
      " [6.27422410e-01 1.97926222e-01 1.74900625e-04]]\n",
      "testing new data result :\n",
      "[answer]  -->  [predict err]\n",
      "back_smash  -->  fo_cut         err_index number :  84\n",
      "hit:  86  miss :  1 percent :  98.85057471264368\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'swing'\n",
    "input_axis = ['AX','AY','AZ']\n",
    "mode_list = ['lstm']\n",
    "power = 4\n",
    "is_aug = 1\n",
    "\n",
    "for mode_name in mode_list:\n",
    "    prac_machine(dir_name,input_axis,mode_name,power,is_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
